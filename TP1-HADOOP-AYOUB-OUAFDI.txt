Windows PowerShell
Copyright (C) Microsoft Corporation. Tous droits réservés.

Installez la dernière version de PowerShell pour de nouvelles fonctionnalités et améliorations ! https://aka.ms/PSWindows

PS C:\WINDOWS\system32> docker version
Client:
 Version:           28.5.1
 API version:       1.51
 Go version:        go1.24.8
 Git commit:        e180ab8
 Built:             Wed Oct  8 12:19:16 2025
 OS/Arch:           windows/amd64
 Context:           desktop-linux

Server: Docker Desktop 4.49.0 (208700)
 Engine:
  Version:          28.5.1
  API version:      1.51 (minimum version 1.24)
  Go version:       go1.24.8
  Git commit:       f8215cc
  Built:            Wed Oct  8 12:17:24 2025
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.7.27
  GitCommit:        05044ec0a9a75232cad458027ca83437aae3f4da
 runc:
  Version:          1.2.5
  GitCommit:        v1.2.5-0-g59923ef
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
PS C:\WINDOWS\system32> docker info
Client:
 Version:    28.5.1
 Context:    desktop-linux
 Debug Mode: false
 Plugins:
  ai: Docker AI Agent - Ask Gordon (Docker Inc.)
    Version:  v1.9.11
    Path:     C:\Program Files\Docker\cli-plugins\docker-ai.exe
  buildx: Docker Buildx (Docker Inc.)
    Version:  v0.29.1-desktop.1
    Path:     C:\Program Files\Docker\cli-plugins\docker-buildx.exe
  compose: Docker Compose (Docker Inc.)
    Version:  v2.40.3-desktop.1
    Path:     C:\Users\A.OUAFDI\.docker\cli-plugins\docker-compose.exe
  debug: Get a shell into any image or container (Docker Inc.)
    Version:  0.0.45
    Path:     C:\Program Files\Docker\cli-plugins\docker-debug.exe
  desktop: Docker Desktop commands (Docker Inc.)
    Version:  v0.2.0
    Path:     C:\Program Files\Docker\cli-plugins\docker-desktop.exe
  extension: Manages Docker extensions (Docker Inc.)
    Version:  v0.2.31
    Path:     C:\Program Files\Docker\cli-plugins\docker-extension.exe
  init: Creates Docker-related starter files for your project (Docker Inc.)
    Version:  v1.4.0
    Path:     C:\Program Files\Docker\cli-plugins\docker-init.exe
  mcp: Docker MCP Plugin (Docker Inc.)
    Version:  v0.24.0
    Path:     C:\Program Files\Docker\cli-plugins\docker-mcp.exe
  model: Docker Model Runner (Docker Inc.)
    Version:  v0.1.46
    Path:     C:\Users\A.OUAFDI\.docker\cli-plugins\docker-model.exe
  offload: Docker Offload (Docker Inc.)
    Version:  v0.5.1
    Path:     C:\Program Files\Docker\cli-plugins\docker-offload.exe
  sandbox: Docker Sandbox (Docker Inc.)
    Version:  v0.3.1
    Path:     C:\Program Files\Docker\cli-plugins\docker-sandbox.exe
  sbom: View the packaged-based Software Bill Of Materials (SBOM) for an image (Anchore Inc.)
    Version:  0.6.0
    Path:     C:\Program Files\Docker\cli-plugins\docker-sbom.exe
  scout: Docker Scout (Docker Inc.)
    Version:  v1.18.3
    Path:     C:\Program Files\Docker\cli-plugins\docker-scout.exe

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 28.5.1
 Storage Driver: overlayfs
  driver-type: io.containerd.snapshotter.v1
 Logging Driver: json-file
 Cgroup Driver: cgroupfs
 Cgroup Version: 2
 Plugins:
  Volume: local
  Network: bridge host ipvlan macvlan null overlay
  Log: awslogs fluentd gcplogs gelf journald json-file local splunk syslog
 CDI spec directories:
  /etc/cdi
  /var/run/cdi
 Discovered Devices:
  cdi: docker.com/gpu=webgpu
 Swarm: inactive
 Runtimes: io.containerd.runc.v2 nvidia runc
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 05044ec0a9a75232cad458027ca83437aae3f4da
 runc version: v1.2.5-0-g59923ef
 init version: de40ad0
 Security Options:
  seccomp
   Profile: builtin
  cgroupns
 Kernel Version: 6.6.87.2-microsoft-standard-WSL2
 Operating System: Docker Desktop
 OSType: linux
 Architecture: x86_64
 CPUs: 16
 Total Memory: 15.51GiB
 Name: docker-desktop
 ID: 9a9770a6-c036-42c6-b3f3-f3d114389fd3
 Docker Root Dir: /var/lib/docker
 Debug Mode: false
 HTTP Proxy: http.docker.internal:3128
 HTTPS Proxy: http.docker.internal:3128
 No Proxy: hubproxy.docker.internal
 Labels:
  com.docker.desktop.address=npipe://\\.\pipe\docker_cli
 Experimental: false
 Insecure Registries:
  hubproxy.docker.internal:5555
  ::1/128
  127.0.0.0/8
 Live Restore Enabled: false

PS C:\WINDOWS\system32> docker pull yassern1/hadoop-spark-jupyter:1.0.3
1.0.3: Pulling from yassern1/hadoop-spark-jupyter
7c457f213c76: Pull complete
62d02b53c336: Pull complete
4f4fb700ef54: Pull complete
40091887436b: Pull complete
93cfcb2f0187: Pull complete
88defd485d1b: Pull complete
e44c72237599: Pull complete
3e7b11a95670: Pull complete
5c336d7045ea: Pull complete
7f61e7eab4ae: Pull complete
873ac057ca4e: Pull complete
70e2e8d7dba3: Pull complete
b551d0588ff7: Pull complete
b71f1f2e032a: Pull complete
fae462c2169c: Pull complete
916b439a2e8b: Pull complete
47fc09f775a8: Pull complete
776a89496dc4: Pull complete
0c7c8e899c31: Pull complete
Digest: sha256:511eec146b04568b392ff0a3522c54b47d7c010181334c96bcc9dbbe17ca2fa3
Status: Downloaded newer image for yassern1/hadoop-spark-jupyter:1.0.3
docker.io/yassern1/hadoop-spark-jupyter:1.0.3
PS C:\WINDOWS\system32> docker images
REPOSITORY                      TAG       IMAGE ID       CREATED       SIZE
yassern1/hadoop-spark-jupyter   1.0.3     511eec146b04   7 weeks ago   3.67GB
PS C:\WINDOWS\system32>  docker network create --driver=bridge hadoop
81cd023463d0b7aeca9671edb87dcf99dd26ab64ad00b4b373b9c086016424c9
PS C:\WINDOWS\system32> docker run -itd -v C:\Users\<ton_nom>\Documents\hadoop_project:/shared_volume `
>> --net=hadoop -p 9870:9870 -p 8088:8088 -p 7077:7077 -p 19888:19888 -p 8080:8080 -p 9000:9000 `
>> --name hadoop-master --hostname hadoop-master yassern1/hadoop-spark-jupyter:1.0.3
docker: Error response from daemon: CreateFile C:\Users\<ton_nom>\Documents\hadoop_project: The filename, directory name, or volume label syntax is incorrect.

Run 'docker run --help' for more information
PS C:\WINDOWS\system32> docker run -itd -v C:\Users\A.OUAFDI\Documents\hadoop_project:/shared_volume `
>> --net=hadoop -p 9870:9870 -p 8088:8088 -p 7077:7077 -p 19888:19888 -p 8080:8080 -p 9000:9000 `
>> --name hadoop-master --hostname hadoop-master yassern1/hadoop-spark-jupyter:1.0.3
c81d45f9a66b0dbaa9c931e6fc5b9e60e9a5748a03589cc5aec1570f13cfa9bb
PS C:\WINDOWS\system32> docker run -itd -p 8040:8042 --net=hadoop --name hadoop-slave1 `
>> --hostname hadoop-slave1 yassern1/hadoop-spark-jupyter:1.0.3
21939f99a702616930a1d380ef048f1eca355732436e865f9df46c23257710a5
PS C:\WINDOWS\system32> docker run -itd -p 8041:8042 --net=hadoop --name hadoop-slave2 `
>> --hostname hadoop-slave2 yassern1/hadoop-spark-jupyter:1.0.3
903433336b5e915fca75d59d7a85de8cc0977b0e4a5330596e112d846966d750
PS C:\WINDOWS\system32> docker ps
CONTAINER ID   IMAGE                                 COMMAND                  CREATED              STATUS              PORTS                                                                                                                                                                                                                                                                              NAMES
903433336b5e   yassern1/hadoop-spark-jupyter:1.0.3   "sh -c 'service ssh …"   13 seconds ago       Up 12 seconds       0.0.0.0:8041->8042/tcp, [::]:8041->8042/tcp                                                                                                                                                                                                                                        hadoop-slave2
21939f99a702   yassern1/hadoop-spark-jupyter:1.0.3   "sh -c 'service ssh …"   About a minute ago   Up About a minute   0.0.0.0:8040->8042/tcp, [::]:8040->8042/tcp                                                                                                                                                                                                                                        hadoop-slave1
c81d45f9a66b   yassern1/hadoop-spark-jupyter:1.0.3   "sh -c 'service ssh …"   About a minute ago   Up About a minute   0.0.0.0:7077->7077/tcp, [::]:7077->7077/tcp, 0.0.0.0:8080->8080/tcp, [::]:8080->8080/tcp, 0.0.0.0:8088->8088/tcp, [::]:8088->8088/tcp, 0.0.0.0:9000->9000/tcp, [::]:9000->9000/tcp, 0.0.0.0:9870->9870/tcp, [::]:9870->9870/tcp, 0.0.0.0:19888->19888/tcp, [::]:19888->19888/tcp   hadoop-master
PS C:\WINDOWS\system32> docker exec -it hadoop-master bash
root@hadoop-master:~# ./start-hadoop.sh




Starting namenodes on [hadoop-master]
hadoop-master: Warning: Permanently added 'hadoop-master,172.18.0.2' (ECDSA) to the list of known hosts.
hadoop-master: WARNING: HADOOP_NAMENODE_OPTS has been replaced by HDFS_NAMENODE_OPTS. Using value of HADOOP_NAMENODE_OPTS.
Starting datanodes
WARNING: HADOOP_SECURE_DN_LOG_DIR has been replaced by HADOOP_SECURE_LOG_DIR. Using value of HADOOP_SECURE_DN_LOG_DIR.
hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts.
hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.4' (ECDSA) to the list of known hosts.
hadoop-slave2: WARNING: HADOOP_SECURE_DN_LOG_DIR has been replaced by HADOOP_SECURE_LOG_DIR. Using value of HADOOP_SECURE_DN_LOG_DIR.
hadoop-slave2: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.
hadoop-slave1: WARNING: HADOOP_SECURE_DN_LOG_DIR has been replaced by HADOOP_SECURE_LOG_DIR. Using value of HADOOP_SECURE_DN_LOG_DIR.
hadoop-slave1: WARNING: HADOOP_DATANODE_OPTS has been replaced by HDFS_DATANODE_OPTS. Using value of HADOOP_DATANODE_OPTS.
Starting secondary namenodes [hadoop-master]
hadoop-master: Warning: Permanently added 'hadoop-master,172.18.0.2' (ECDSA) to the list of known hosts.
hadoop-master: WARNING: HADOOP_SECONDARYNAMENODE_OPTS has been replaced by HDFS_SECONDARYNAMENODE_OPTS. Using value of HADOOP_SECONDARYNAMENODE_OPTS.
Starting resourcemanager
Starting nodemanagers
hadoop-slave1: Warning: Permanently added 'hadoop-slave1,172.18.0.3' (ECDSA) to the list of known hosts.
hadoop-slave2: Warning: Permanently added 'hadoop-slave2,172.18.0.4' (ECDSA) to the list of known hosts.


root@hadoop-master:~# hdfs dfs -mkdir -p /user/root
root@hadoop-master:~# hdfs dfs -mkdir input
root@hadoop-master:~# hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 /user
root@hadoop-master:~# hdfs dfs -put /shared_volume/purchases.txt .
put: `/shared_volume/purchases.txt': No such file or directory
root@hadoop-master:~# hdfs dfs -ls -R
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 input
root@hadoop-master:~# hdfs dfs -cat purchases.txt
cat: `purchases.txt': No such file or directory
root@hadoop-master:~# hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 input
root@hadoop-master:~#  hdfs dfs -ls -R -h ./
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 input
root@hadoop-master:~# ls /shared_volume
root@hadoop-master:~# hdfs dfs -cat purchases.txt
cat: `purchases.txt': No such file or directory
root@hadoop-master:~# ls /shared_volume
purchases.txt
root@hadoop-master:~# hdfs dfs -put /shared_volume/purchases.txt
root@hadoop-master:~# hdfs dfs -ls -R
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 input
-rw-r--r--   2 root supergroup          0 2025-11-02 19:56 purchases.txt
root@hadoop-master:~#  hdfs dfs -cat purchases.txt
root@hadoop-master:~# hdfs dfs -tail purchases.txt
root@hadoop-master:~#  hdfs dfs -rm purchases.txt
Deleted purchases.txt
root@hadoop-master:~# hdfs dfs -copyFromLocal /shared_volume/purchases.txt ./input
root@hadoop-master:~#  hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~# hdfs dfs -copyFromLocal /shared_volume/purchases.txt ./input
copyFromLocal: `input/purchases.txt': File exists
root@hadoop-master:~#  hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~#  hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~#  hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~#  hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~# ls /shared_volume
purchases.txt
root@hadoop-master:~# hdfs dfs -ls
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 input
root@hadoop-master:~# hdfs dfs -ls /
Found 1 items
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 /user
root@hadoop-master:~# hdfs dfs -ls -R /
drwxr-xr-x   - root supergroup          0 2025-11-02 19:43 /user
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 /user/root
drwxr-xr-x   - root supergroup          0 2025-11-02 19:58 /user/root/input
-rw-r--r--   2 root supergroup          0 2025-11-02 19:58 /user/root/input/purchases.txt
root@hadoop-master:~# hdfs dfs -copyFromLocal /shared_volume/purchases.txt /input
root@hadoop-master:~# hdfs dfs -ls /input
-rw-r--r--   2 root supergroup          0 2025-11-02 20:06 /input
root@hadoop-master:~# hdfs dfs -ls /user/root/input
Found 1 items
-rw-r--r--   2 root supergroup          0 2025-11-02 19:58 /user/root/input/purchases.txt
root@hadoop-master:~# hdfs dfs -ls input
Found 1 items
-rw-r--r--   2 root supergroup          0 2025-11-02 19:58 input/purchases.txt
root@hadoop-master:~#  hdfs dfs -chmod 777 ./input/purchases.txt
root@hadoop-master:~# hdfs dfs -chmod ugo-x ./input/purchases.txt
root@hadoop-master:~# hdfs dfs -mv /input/purchases.txt
-mv: Not enough arguments: expected 2 but got 1
Usage: hadoop fs [generic options]
        [-appendToFile <localsrc> ... <dst>]
        [-cat [-ignoreCrc] <src> ...]
        [-checksum <src> ...]
        [-chgrp [-R] GROUP PATH...]
        [-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]
        [-chown [-R] [OWNER][:[GROUP]] PATH...]
        [-copyFromLocal [-f] [-p] [-l] [-d] [-t <thread count>] <localsrc> ... <dst>]
        [-copyToLocal [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-count [-q] [-h] [-v] [-t [<storage type>]] [-u] [-x] [-e] <path> ...]
        [-cp [-f] [-p | -p[topax]] [-d] <src> ... <dst>]
        [-createSnapshot <snapshotDir> [<snapshotName>]]
        [-deleteSnapshot <snapshotDir> <snapshotName>]
        [-df [-h] [<path> ...]]
        [-du [-s] [-h] [-v] [-x] <path> ...]
        [-expunge]
        [-find <path> ... <expression> ...]
        [-get [-f] [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]
        [-getfacl [-R] <path>]
        [-getfattr [-R] {-n name | -d} [-e en] <path>]
        [-getmerge [-nl] [-skip-empty-file] <src> <localdst>]
        [-head <file>]
        [-help [cmd ...]]
        [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [-e] [<path> ...]]
        [-mkdir [-p] <path> ...]
        [-moveFromLocal <localsrc> ... <dst>]
        [-moveToLocal <src> <localdst>]
        [-mv <src> ... <dst>]
        [-put [-f] [-p] [-l] [-d] <localsrc> ... <dst>]
        [-renameSnapshot <snapshotDir> <oldName> <newName>]
        [-rm [-f] [-r|-R] [-skipTrash] [-safely] <src> ...]
        [-rmdir [--ignore-fail-on-non-empty] <dir> ...]
        [-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]
        [-setfattr {-n name [-v value] | -x name} <path>]
        [-setrep [-R] [-w] <rep> <path> ...]
        [-stat [format] <path> ...]
        [-tail [-f] <file>]
        [-test -[defsz] <path>]
        [-text [-ignoreCrc] <src> ...]
        [-touch [-a] [-m] [-t TIMESTAMP ] [-c] <path> ...]
        [-touchz <path> ...]
        [-truncate [-w] <length> <path> ...]
        [-usage [cmd ...]]

Generic options supported are:
-conf <configuration file>        specify an application configuration file
-D <property=value>               define a value for a given property
-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.
-jt <local|resourcemanager:port>  specify a ResourceManager
-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster
-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath
-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines

The general command line syntax is:
command [genericOptions] [commandOptions]

Usage: hadoop fs [generic options] -mv <src> ... <dst>
root@hadoop-master:~# hdfs dfs -mv ./input/purchases.txt ./purchases.txt
root@hadoop-master:~#  hdfs dfs -ls -R
drwxr-xr-x   - root supergroup          0 2025-11-02 20:12 input
-rw-rw-rw-   2 root supergroup          0 2025-11-02 19:58 purchases.txt
root@hadoop-master:~# hdfs dfs -get ./input/purchases.txt /shared_volume/achat.txt
get: `./input/purchases.txt': No such file or directory
root@hadoop-master:~# hdfs dfs -get ./purchases.txt /shared_volume/achat.txt
root@hadoop-master:~# hdfs dfs -cp ./purchases.txt ./input/
root@hadoop-master:~# hdfs dfs -ls input
Found 1 items
-rw-r--r--   2 root supergroup          0 2025-11-02 20:23 input/purchases.txt
root@hadoop-master:~# hdfs dfs -mkdir web_input
root@hadoop-master:~# hdfs dfs -ls
Found 3 items
drwxr-xr-x   - root supergroup          0 2025-11-02 20:23 input
-rw-rw-rw-   2 root supergroup          0 2025-11-02 19:58 purchases.txt
drwxr-xr-x   - root supergroup          0 2025-11-02 20:24 web_input
root@hadoop-master:~# wget http://www.textfiles.com/etext/FICTION/alice.txt
--2025-11-02 20:26:07--  http://www.textfiles.com/etext/FICTION/alice.txt
Resolving www.textfiles.com (www.textfiles.com)... 208.86.224.90
Connecting to www.textfiles.com (www.textfiles.com)|208.86.224.90|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 150886 (147K) [text/plain]
Saving to: 'alice.txt'

alice.txt                                                   100%[========================================================================================================================================>] 147.35K   200KB/s    in 0.7s

2025-11-02 20:26:09 (200 KB/s) - 'alice.txt' saved [150886/150886]

root@hadoop-master:~# ls
alice.txt  hdfs  run-wordcount.sh  start-hadoop.sh  start-jupyter.sh  start-kafka-zookeeper.sh  start-spark.sh
root@hadoop-master:~# cp alice.txt /shared_volume/
root@hadoop-master:~# ls /shared_volume
achat.txt  alice.txt  purchases.txt
root@hadoop-master:~# hdfs dfs -put /shared_volume/alice.txt web_input
root@hadoop-master:~# hdfs dfs -ls web_input
Found 1 items
-rw-r--r--   2 root supergroup     150886 2025-11-02 20:27 web_input/alice.txt
root@hadoop-master:~# hdfs dfs -cat web_input/alice.txt | head -n 20
          PROJECT GUTENBERG AND DUNCAN RESEARCH SHAREWARE

                             (c)1991

Project Gutenberg has made arrangements with Duncan Research for
the distribution of Duncan Research Electronic Library text.  No
money is solicited by Project Gutenberg.  All donations go to:

Barbara  Duncan
Duncan Research
P.O.  Box  2782
Champaign,   IL
61825 - 2782

Please, if you send in a request for information, donate enough,
or more than enough to cover the cost of writing, printing, etc.
as well as the cost of postage.

This is Shareware, you may post it intact anywhere, as long as a
profit is not incurred.
cat: Unable to write to output stream.
root@hadoop-master:~# hdfs dfs -tail web_input/alice.txt
he pool rippling to the waving of the
reeds--the rattling teacups would change to tinkling sheep-
bells, and the Queen's shrill cries to the voice of the shepherd
boy--and the sneeze of the baby, the shriek of the Gryphon, and
all thy other queer noises, would change (she knew) to the
confused clamour of the busy farm-yard--while the lowing of the
cattle in the distance would take the place of the Mock Turtle's
heavy sobs.

  Lastly, she pictured to herself how this same little sister of
hers would, in the after-time, be herself a grown woman; and how
she would keep, through all her riper years, the simple and
loving heart of her childhood:  and how she would gather about
her other little children, and make THEIR eyes bright and eager
with many a strange tale, perhaps even with the dream of
Wonderland of long ago:  and how she would feel with all their
simple sorrows, and find a pleasure in all their simple joys,
remembering her own child-life, and the happy summer days.

                             THE END
root@hadoop-master:~# exit
exit
PS C:\WINDOWS\system32> Get-History | Out-File -FilePath "C:\Users\Ayoub\Documents\historique_powershell.txt"
Out-File : Impossible de trouver une partie du chemin d'accès 'C:\Users\Ayoub\Documents\historique_powershell.txt'.
Au caractère Ligne:1 : 15
+ ... t-History | Out-File -FilePath "C:\Users\Ayoub\Documents\historique_p ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : OpenError: (:) [Out-File], DirectoryNotFoundException
    + FullyQualifiedErrorId : FileOpenFailure,Microsoft.PowerShell.Commands.OutFileCommand

PS C:\WINDOWS\system32> ^C
PS C:\WINDOWS\system32> Get-History | Out-File -FilePath "C:\Users\Ayoub\Documents\historique_powershell.txt"
Out-File : Impossible de trouver une partie du chemin d'accès 'C:\Users\Ayoub\Documents\historique_powershell.txt'.
Au caractère Ligne:1 : 15
+ ... t-History | Out-File -FilePath "C:\Users\Ayoub\Documents\historique_p ...
+                 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : OpenError: (:) [Out-File], DirectoryNotFoundException
    + FullyQualifiedErrorId : FileOpenFailure,Microsoft.PowerShell.Commands.OutFileCommand

PS C:\WINDOWS\system32> Get-History | Out-File -FilePath "C:\Users\A.OUAFDI\Documents\historique_powershell.txt"
PS C:\WINDOWS\system32> Start-Transcript -Path "C:\Users\Ayoub\Documents\session_powershell.txt"
Transcription démarrée, le fichier de sortie est C:\Users\Ayoub\Documents\session_powershell.txt
PS C:\WINDOWS\system32> Start-Transcript -Path "C:\Users\A.OUAFDI\Documents\session_powershell.txt"
Transcription démarrée, le fichier de sortie est C:\Users\A.OUAFDI\Documents\session_powershell.txt
PS C:\WINDOWS\system32> docker stop hadoop-master hadoop-slave1 hadoop-slave2
hadoop-master
hadoop-slave1
hadoop-slave2
PS C:\WINDOWS\system32>