{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa8MXXm/XpK5TOtRgslZcy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayoubouafdi/BigDataLabs/blob/main/TP_Spark_Streaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqnGx2tzBHCT",
        "outputId": "f773430c-937a-46ef-ee6d-29fb18fc1f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n",
            "Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Création du dossier qui va recevoir les fichiers texte\n",
        "os.makedirs(\"/content/mon_flux/\", exist_ok=True)\n",
        "import socket, time, threading\n",
        "\n",
        "def start_socket_server():\n",
        "    host = \"localhost\"\n",
        "    port = 9999\n",
        "    s = socket.socket()\n",
        "    s.bind((host, port))\n",
        "    s.listen(1)\n",
        "    conn, addr = s.accept()\n",
        "    messages = [\"spark streaming dstream\", \"spark spark streaming\", \"big data spark\"]\n",
        "    while True:\n",
        "        for msg in messages:\n",
        "            conn.send((msg + \"\\n\").encode())\n",
        "            time.sleep(2)\n",
        "\n",
        "# Lancement du serveur dans un thread séparé\n",
        "threading.Thread(target=start_socket_server, daemon=True).start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZKFs2GRBVRm",
        "outputId": "255dad80-d5c1-4c17-b434-423909401c93"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception in thread Thread-17 (start_socket_server):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/tmp/ipython-input-3035698602.py\", line 10, in start_socket_server\n",
            "OSError: [Errno 98] Address already in use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "ssc = StreamingContext(sc, 5) # 5 secondes est la durée du batch"
      ],
      "metadata": {
        "id": "pb1h9NnPBcFn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lire depuis le socket local sur le port 9999.\n",
        "lines = ssc.textFileStream(\"/content/mon_flux/\")\n",
        "\n",
        "#Détailler les lignes en mots (flatMap).\n",
        "words = lines.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "#Transformer chaque mot en paire (mot, 1)\n",
        "pairs = words.map(lambda w: (w, 1))\n",
        "\n",
        "#Réduire par clé pour additionner les occurrences.\n",
        "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "#Affichage des résultats\n",
        "counts.pprint()"
      ],
      "metadata": {
        "id": "i3oJ3vTcBryX"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ssc.start()\n",
        "ssc.awaitTerminationOrTimeout(30)\n",
        "ssc.stop(stopSparkContext=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Pz24XrMCcMX",
        "outputId": "96129c8a-5f41-44bd-a88f-40c6663cc1d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:25\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:30\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:35\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:40\n",
            "-------------------------------------------\n",
            "('AYOUB', 1)\n",
            "('OUAFDI', 1)\n",
            "('IDS', 1)\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:45\n",
            "-------------------------------------------\n",
            "\n",
            "-------------------------------------------\n",
            "Time: 2026-01-27 20:16:50\n",
            "-------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# RÉPONSES AUX QUESTIONS DU TP - PAR AYOUB\n",
        "\n",
        "\n",
        "# 1 et 2. Organisation du TP\n",
        "# Le TP est divisé en sections : installation, serveur socket,\n",
        "# création du contexte Spark, traitement WordCount et exécution.\n",
        "# Chaque partie du code est commentée pour expliquer l'étape.\n",
        "\n",
        "# 3. Modification pour la lecture depuis un fichier\n",
        "# Pour lire des fichiers au lieu d'un socket, j'ai remplacé :\n",
        "# lines = ssc.socketTextStream(\"localhost\", 9999)\n",
        "# par :\n",
        "# lines = ssc.textFileStream(\"/content/mon_flux/\")\n",
        "# Cela permet à Spark de scanner un dossier et de traiter les nouveaux fichiers.\n",
        "\n",
        "# 4. Qu'est-ce qu'un micro-batch ?\n",
        "# C'est la façon dont Spark Streaming travaille : il ne traite pas les\n",
        "# données une par une, mais il attend quelques secondes pour regrouper\n",
        "# les données reçues dans un petit lot (batch) avant de les calculer.\n",
        "\n",
        "# 5. Structure de base d'un DStream\n",
        "# Un DStream est en fait une suite continue de RDD (Resilient Distributed Datasets).\n",
        "# Chaque RDD représente les données arrivées pendant l'intervalle du batch.\n",
        "\n",
        "# 6. Durée du batch\n",
        "# Dans ce TP, on a utilisé une durée de 5 secondes. C'est le temps que Spark\n",
        "# attend avant de traiter les mots reçus et d'afficher le résultat.\n"
      ],
      "metadata": {
        "id": "rPLziczYCqCl"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}